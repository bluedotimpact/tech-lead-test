Resource - Unit,[>] URL,Time (mins),Status,[>] Audio URL,[>] Authors,[>] Type,[>] Year,[>] Resource name,Order,Guide,[>] Chunk,[>] Course (from [>] Chunk),[>] Unit (from Chunk)
Seeking Stability in the Competition for AI Advantage - ,https://www.rand.org/pubs/commentary/2025/03/seeking-stability-in-the-competition-for-ai-advantage.html,15,Core,,"Rehman, Mueller, and Mazarr",Article,2025,Seeking Stability in the Competition for AI Advantage,1,"This RAND article describes some of the international dynamics driving the race to AGI between the US and China, and analyses whether nuclear deterrence logic applies to this race.",Steering the race to AGI,AGI Strategy,AGI Strategy - 1: Racing to a Better Future
Preparing for Launch - ,https://ifp.org/preparing-for-launch/,30,Core,,"Tim Fist, Tao Burga, and Tim Hwang",Blog,2025,Preparing for Launch,1,The Institute for Progress lays out how the US Government could shape the development of AI towards human flourishing by accelerating beneficial AI applications and defences against societal harms.,Imagining a better future,AGI Strategy,AGI Strategy - 1: Racing to a Better Future
It’s practically impossible to run a big AI company ethically - ,https://archive.ph/9DCPq,15,Core,,Sigal Samuel,Article,2024,It’s practically impossible to run a big AI company ethically,2,"Even ""safety-first"" AI companies like Anthropic face market pressure that can override ethical commitments. This article demonstrates the constraints facing AI companies, and why voluntary corporate governance can't solve coordination problems alone. ",Steering the race to AGI,AGI Strategy,AGI Strategy - 1: Racing to a Better Future
The Gentle Singularity - ,https://blog.samaltman.com/the-gentle-singularity,10,Core,,Sam Altman ,Blog,2025,The Gentle Singularity,2,"This blog post offers a vivid, optimistic vision of rapid AI progress from the CEO of OpenAI. Altman suggests that the accelerating technological change will feel ""impressive but manageable,"" and that there are serious challenges to confront.",Imagining a better future,AGI Strategy,AGI Strategy - 1: Racing to a Better Future
Solarpunk: A Vision for a Sustainable Future - ,https://newintrigue.com/2025/01/29/solarpunk-a-vision-for-a-sustainable-future/,10,Core,,Joshua Krook,Blog,2025,Solarpunk: A Vision for a Sustainable Future,3,"What might sustainable human progress look like, beyond pure technological acceleration? This essay provides an alternative vision, based on communities living in greater harmony with each other and with nature, alongside advanced technologies. ",Imagining a better future,AGI Strategy,AGI Strategy - 1: Racing to a Better Future
In search of a dynamist vision for safe superhuman AI - ,https://helentoner.substack.com/p/dynamism-vs-stasis,15,Core,,Helen Toner,Blog,2025,In search of a dynamist vision for safe superhuman AI,3,"This essay describes AI safety policies that rely on centralised control (surveillance, fewer AI projects, licensing regimes) as ""stasist"" approaches that sacrifice innovation for stability. Toner argues we need ""dynamist"" solutions to the risks from AI that allow for decentralised experimentation, creativity and risk-taking. ",Steering the race to AGI,AGI Strategy,AGI Strategy - 1: Racing to a Better Future
The AI Revolution: The Road to Superintelligence - ,https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html,25,Core,,Tim Urban,Blog,2015,The AI Revolution: The Road to Superintelligence,1,"Tim Urban uses vivid historical analogies to show why AI progress accelerates much faster than we intuitively expect, building toward the concept of an ""intelligence explosion"" where AI systems could rapidly self-improve from human-level to superintelligent - a scenario that could fundamentally shift global power dynamics.",Intelligence explosion,AGI Strategy,AGI Strategy - 2: The Rapid Proliferation of Power
Scaling: The State of Play in AI - ,https://www.oneusefulthing.org/p/scaling-the-state-of-play-in-ai,15,Core,,Ethan Mollick,,2024,Scaling: The State of Play in AI,1,"This post explains the ""scaling laws"" that drive rapid AI progress: when you make AI models bigger and train them with more computing power, they get smarter at most tasks. The piece also introduces a second scaling law, where AI performance improves by spending more time ""thinking"" before responding.",Technical trends driving AI progress,AGI Strategy,AGI Strategy - 2: The Rapid Proliferation of Power
Governance of superintelligence - ,https://openai.com/index/governance-of-superintelligence/,5,Core,,"Sam Altman, Greg Brockman, Ilya Sutskever",,2023,Governance of superintelligence,1,"OpenAI's leadership outline how humanity might govern superintelligence, proposing international oversight with inspection powers similar to nuclear regulation. They argue the AI systems arriving this decade will be ""more powerful than any technology yet created"" and their control cannot be left to individual companies alone.",What should we do with all this power?,AGI Strategy,AGI Strategy - 2: The Rapid Proliferation of Power
Trends in AI: Training Costs and Diffusion Speed - ,https://dewierwan.github.io/ai-training-diffusion,10,Core,,Dewi Erwan ,Website,2025,Trends in AI: Training Costs and Diffusion Speed,2,"Use these graphs to analyse how much compute is being used to train frontier AI models, and how quickly those capabilities diffuse due to improvements in hardware and algorithms. ",Technical trends driving AI progress,AGI Strategy,AGI Strategy - 2: The Rapid Proliferation of Power
The Most Important Time in History Is Now - ,https://unchartedterritories.tomaspueyo.com/p/the-most-important-time-in-history-agi-asi,30,Core,,Tomas Pueyo,Blog,2025,The Most Important Time in History Is Now,2,"This blog post traces AI's rapid leap from high school to PhD-level intelligence in just two years, examines whether physical bottlenecks like computing power can slow this acceleration, and argues that recent efficiency breakthroughs suggest we're entering an intelligence explosion toward artificial general intelligence within years.",Intelligence explosion,AGI Strategy,AGI Strategy - 2: The Rapid Proliferation of Power
d/acc: one year later - ,https://vitalik.eth.limo/general/2025/01/05/dacc2.html,30,Core,,Vitalik Buterin,,2025,d/acc: one year later,2,"Ethereum founder Vitalik Buterin describes how democratic, defensive and decentralised technologies could distribute AI's power across society rather than concentrating it, offering a middle path between unchecked technical acceleration and authoritarian control.",What should we do with all this power?,AGI Strategy,AGI Strategy - 2: The Rapid Proliferation of Power
"""""""Long"""" timelines to advanced AI have gotten crazy short"" - ",https://helentoner.substack.com/p/long-timelines-to-advanced-ai-have,10,Core,,Helen Toner,,2025,"""Long"" timelines to advanced AI have gotten crazy short",3,"Helen Toner, former OpenAI board member, reveals how the entire AI timeline debate has compressed: even conservative experts who once dismissed advanced AI concerns now predict human-level systems within decades, demonstrating that rapid AI progress has shifted from a fringe prediction to mainstream expert consensus.",Intelligence explosion,AGI Strategy,AGI Strategy - 2: The Rapid Proliferation of Power
Measuring AI Ability to Complete Long Tasks - ,https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/,10,Core,,Thomas Kwa et al.,,2025,Measuring AI Ability to Complete Long Tasks,4,,Intelligence explosion,AGI Strategy,AGI Strategy - 2: The Rapid Proliferation of Power
Catastrophic AI Scenarios - ,https://futureoflife.org/resource/catastrophic-ai-scenarios/,5,Core,,Ben Eisenpress,,2024,Catastrophic AI Scenarios,1,"The Future of Life Institute show how existing AI can already help to design bioweapons, amplify cyberattacks, and deceive human. Each threat is backed by existing evidence. This gives you concrete examples to draw from when mapping your own threat scenarios.",Map the threat landscape,AGI Strategy,AGI Strategy - 3: Pathways to Harm
We're Not Ready for Superintelligence (AI 2027 Summary) - ,https://www.youtube.com/watch?v=5KVDDfAkRgc,35,Core,,Aric Floyd,,2025,We're Not Ready for Superintelligence (AI 2027 Summary),1,,Option 1: Power concentration,AGI Strategy,AGI Strategy - 3: Pathways to Harm
The Future of Phishing - ,https://civai.org/p/email-phishing,10,Core,,CivAI,,2025,The Future of Phishing,1,,Option 2: Critical infrastructure attacks,AGI Strategy,AGI Strategy - 3: Pathways to Harm
AI and the Evolution of Biological National Security Risks - ,https://www.cnas.org/publications/reports/ai-and-the-evolution-of-biological-national-security-risks,15,Core,,Bill Drexel and Caleb Withers,,2024,AI and the Evolution of Biological National Security Risks,1,,Option 3: Pandemics,AGI Strategy,AGI Strategy - 3: Pathways to Harm
Reframing AGI Threat Models - ,https://www.youtube.com/watch?v=4v3uqWeVmco,5,Core,,Richard Ngo,,2024,Reframing AGI Threat Models,2,"In this talk, Richard Ngo argues that ""misalignment"" and ""misuse"" are two sides of the same coin, and governance and technical interventions against examples of misalignment and misuse are frequently the same. ",Map the threat landscape,AGI Strategy,AGI Strategy - 3: Pathways to Harm
"""CBRN evaluations, Claude Opus 4 System Card"" - ",https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=88,15,Core,,Anthropic,Paper,2025,"CBRN evaluations, Claude Opus 4 System Card",2,Skim pages 88 to 103. ,Option 3: Pandemics,AGI Strategy,AGI Strategy - 3: Pathways to Harm
‘3cb’: The Catastrophic Cyber Capabilities Benchmark - ,https://apartresearch.com/news/3cb-the-catastrophic-cyber-capabilities-benchmark,10,Core,,Jonathan Ng,,2024,‘3cb’: The Catastrophic Cyber Capabilities Benchmark,2,,Option 2: Critical infrastructure attacks,AGI Strategy,AGI Strategy - 3: Pathways to Harm
AI-Enabled Coups: How a Small Group Could Use AI to Seize Power - ,https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power,15,Core,https://open.spotify.com/episode/3IYS3SQxNfKgA4XW3tFTKD?si=s9eFTD1fRPicV50M0CNaIg,"Tom Davidson, Lukas Finnveden, Rose Hadshar",Blog,2025,AI-Enabled Coups: How a Small Group Could Use AI to Seize Power,2,"Read section 4 on ""Concrete paths to an AI-enabled coup"".",Option 1: Power concentration,AGI Strategy,AGI Strategy - 3: Pathways to Harm
AI Could Defeat All Of Us Combined - ,https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/,25,Core,https://open.spotify.com/episode/4reDpdXTUT9u25lEBqZ8HI?si=5a46eb9b836a4b5a,Holden Karnofsky,Article,2022,AI Could Defeat All Of Us Combined,3,"Holden Karnofsky (member of technical staff at Anthropic, founder of GiveWell and the philanthropic foundation Open Philanthropy) describes how human-level AI could spawn hundreds of millions of copies of itself that coordinate to physically defeat humanity. They might accumulate resources, recruit allies, and develop weapons to overpower civilisation. This provides a concrete pathway showing how AI doesn't need to be superintelligent to threaten humanity; it just needs to outnumber us while working as a unified force against us.",Map the threat landscape,AGI Strategy,AGI Strategy - 3: Pathways to Harm
Cyber attacks on critical infrastructure - ,https://commercial.allianz.com/news-and-insights/expert-risk-articles/cyber-attacks-on-critical-infrastructure.html,10,Core,,Allianz Insurance,,2016,Cyber attacks on critical infrastructure,3,,Option 2: Critical infrastructure attacks,AGI Strategy,AGI Strategy - 3: Pathways to Harm
AI-Enabled Coups: How a Small Group Could Use AI to Seize Power - ,https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power,15,Core,https://open.spotify.com/episode/3IYS3SQxNfKgA4XW3tFTKD?si=s9eFTD1fRPicV50M0CNaIg,"Tom Davidson, Lukas Finnveden, Rose Hadshar",Blog,2025,AI-Enabled Coups: How a Small Group Could Use AI to Seize Power,4,"Read the summary and the intro.

In this blog post, researchers at the Forethought Foundation describe how advanced AI could enable a single person to overthrow governments without any human supporters, through three mechanisms: military AI systems programmed with unwavering loyalty to one leader, secretly loyal AI that passes security tests but executes coups when deployed, and monopolistic access to superhuman capabilities in weapons development and cyber warfare. ",Map the threat landscape,AGI Strategy,AGI Strategy - 3: Pathways to Harm
Most common AI-powered cyberattacks - ,https://www.crowdstrike.com/en-us/cybersecurity-101/cyberattacks/ai-powered-cyberattacks/,10,Core,,Crowdstrike,Blog,2025,Most common AI-powered cyberattacks,5,"CrowdStrike, an American cybersecurity company, explains how attackers use AI to automate reconnaissance, craft personalised phishing at scale, and adapt in real-time to evade detection - capabilities that make infrastructure attacks faster and harder to stop than traditional cyberattacks.",Map the threat landscape,AGI Strategy,AGI Strategy - 3: Pathways to Harm
AI Is Reviving Fears Around Bioterrorism. What’s the Real Risk? - ,https://www.cigionline.org/articles/ai-is-reviving-fears-around-bioterrorism-whats-the-real-risk/,5,Core,,Kyle Hiebert,,2025,AI Is Reviving Fears Around Bioterrorism. What’s the Real Risk?,6,"Security experts document how AI chatbots can now guide non-experts through creating bioweapons from readily available ingredients, eliminating the technical complexity that has historically prevented terrorists and apocalyptic groups from mounting successful biological attacks.",Map the threat landscape,AGI Strategy,AGI Strategy - 3: Pathways to Harm
Can Democracy Survive the Disruptive Power of AI? - ,https://carnegieendowment.org/research/2024/12/can-democracy-survive-the-disruptive-power-of-ai?lang=en,15,Maybe,,Raluca Csernatoni,Blog,2024,Can Democracy Survive the Disruptive Power of AI?,,,Map the threat landscape,AGI Strategy,AGI Strategy - 3: Pathways to Harm
AI-assisted targeting in the Gaza Strip - ,https://www.wikiwand.com/en/articles/AI-assisted_targeting_in_the_Gaza_Strip,20,Maybe,,Wiki,Article,2025,AI-assisted targeting in the Gaza Strip,,,Map the threat landscape,AGI Strategy,AGI Strategy - 3: Pathways to Harm
"""CBRN evaluations, Claude Opus 4 System Card"" - ",https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf#page=88,20,Maybe,,Anthropic,Paper,2025,"CBRN evaluations, Claude Opus 4 System Card",,,Map the threat landscape,AGI Strategy,AGI Strategy - 3: Pathways to Harm
The AI Revolution: Our Immortality or Extinction - ,https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html,80,Maybe,,Tim Urban,,2025,The AI Revolution: Our Immortality or Extinction,2,,Option 1: Power concentration,AGI Strategy,AGI Strategy - 3: Pathways to Harm
Evaluating Frontier Models for Stealth and Situational Awareness - ,https://arxiv.org/abs/2505.01420,,Maybe,,Phuong et al.,,2025,Evaluating Frontier Models for Stealth and Situational Awareness,3,,Option 1: Power concentration,AGI Strategy,AGI Strategy - 3: Pathways to Harm
Alignment faking in large language models - ,https://www.anthropic.com/research/alignment-faking,,Maybe,,Greenblatt et al.,Blog,2024,Alignment faking in large language models,4,,Option 1: Power concentration,AGI Strategy,AGI Strategy - 3: Pathways to Harm
"""The Project, Situational Awareness"" - ",https://situational-awareness.ai/the-project/,25,Core,,Leopold Aschenbrenner,Blog,2024,"The Project, Situational Awareness",1,"A former OpenAI researcher argues that private AI companies cannot safely develop superintelligence due to security vulnerabilities and competitive pressures that override safety. He argues that a government-led 'AGI Project' is inevitable and necessary to prevent adversaries stealing the AI systems, or losing human control over the technology.",Preventing dangerous AI training,AGI Strategy,AGI Strategy - 4: Defence in Depth
Introduction to Mechanistic Interpretability - ,https://bluedot.org/blog/introduction-to-mechanistic-interpretability,10,Core,https://open.spotify.com/episode/39zQUsDIrv4K6p1wPmbrYL?si=017ac3afc6ef4729,Sarah Hastings-Woodhouse,Article,2024,Introduction to Mechanistic Interpretability,1,An introduction to mechanistic interpretability (techniques for understanding AI internal reasoning) that shows how researchers could detect when models are deceiving users or cutting corners to achieve goals.,Constraining dangerous AI capabilities,AGI Strategy,AGI Strategy - 4: Defence in Depth
Resilience and Adaptation to Advanced AI - ,https://airesilience.substack.com/p/resilience-and-adaptation-to-advanced,10,Core,,Jamie Bernardi,Blog,2024,Resilience and Adaptation to Advanced AI,1,"Jamie Bernardi argues that we can't rely solely on model safeguards to ensure AI safety. Instead, he proposes ""AI resilience"": building society's capacity to detect misuse, defend against harmful AI applications, and reduce the damage caused when dangerous AI capabilities spread beyond a government or company's control.",Withstanding dangerous AI actions,AGI Strategy,AGI Strategy - 4: Defence in Depth
What is AI alignment? - ,https://bluedot.org/blog/what-is-ai-alignment,15,Core,https://open.spotify.com/episode/3NjZstGnWnGUrX99GZ9RMk?si=05fc5eccb7f94ec1,Adam Jones,Article,2024,What is AI alignment?,2,"AI systems regularly do things their creators never intended, from maze-solving AIs that get stuck in corners to social media algorithms promoting extremist content. In this blog post, Adam Jones explains how outer alignment (setting correct goals) and inner alignment (ensuring AI follows those goals) might help to prevent these failures as systems become more powerful.",Constraining dangerous AI capabilities,AGI Strategy,AGI Strategy - 4: Defence in Depth
Urging an International AI Treaty: An Open Letter - ,https://aitreaty.org/,5,Core,,Misc,Website,2024,Urging an International AI Treaty: An Open Letter,2,"Leading AI scientists propose an international treaty that would cap the computing power used to train AI models and create a collaborative AI safety laboratory (""CERN for AI""). This is one model for how governments could coordinate to prevent the development of potentially catastrophic AI systems.",Preventing dangerous AI training,AGI Strategy,AGI Strategy - 4: Defence in Depth
AI Emergency Preparedness: Examining the federal government's ability to detect and respond to AI-related national security threats - ,https://arxiv.org/abs/2407.17347,10,Core,,Wasil et al.,,2024,AI Emergency Preparedness: Examining the federal government's ability to detect and respond to AI-related national security threats,2,"Read the Abstract, Policy Recommendations, and Introduction.

This paper uses scenario planning to show how governments could prepare for AI emergencies. The authors examine three plausible disasters: 1) losing control of AI, 2) AI model theft, and 3) bioweapon creation. They then expose gaps in current preparedness systems, and propose specific government reforms including embedding auditors inside AI companies and creating emergency response units.",Withstanding dangerous AI actions,AGI Strategy,AGI Strategy - 4: Defence in Depth
Introduction to AI Control - ,https://bluedot.org/blog/ai-control,10,Core,,Sarah Hastings-Woodhouse,Blog,2025,Introduction to AI Control,3,"This blog post explains why it might be easier to build walls around dangerous AI systems than to make them genuinely care about human welfare. However, these may only be temporary fixes before AI becomes too powerful to control.",Constraining dangerous AI capabilities,AGI Strategy,AGI Strategy - 4: Defence in Depth
Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path? - ,https://arxiv.org/abs/2502.15657,15,Core,,Bengio et al.,,2025,Superintelligent Agents Pose Catastrophic Risks: Can Scientist AI Offer a Safer Path?,3,"Read the abstract and executive summary (pages 1 to 7).

Yoshua Bengio (the world's most cited computer scientist) and other leading AI researchers describe how today's AI training methods systematically produce dangerous behaviours. They then propose 'Scientist AI' as an alternative path: non-agentic AI systems designed to understand rather than act.",Preventing dangerous AI training,AGI Strategy,AGI Strategy - 4: Defence in Depth
A Playbook for Securing AI Model Weights - ,https://www.rand.org/pubs/research_briefs/RBA2849-1.html,15,Core,,Nevo et al.,Blog,2024,A Playbook for Securing AI Model Weights,4,"In this report, RAND researchers identify real-world attack methods that malicious actors could use to steal AI model weights. They propose a five-level security framework that AI companies could implement to defend against different threats, from amateur hackers to nation-state operations.",Constraining dangerous AI capabilities,AGI Strategy,AGI Strategy - 4: Defence in Depth
Alignment faking in large language models - ,https://www.anthropic.com/research/alignment-faking,,Maybe,,Greenblatt et al.,Blog,2024,Alignment faking in large language models,,,Constraining dangerous AI capabilities,AGI Strategy,AGI Strategy - 4: Defence in Depth
Can we scale human feedback for complex AI tasks? - ,https://aisafetyfundamentals.com/blog/scalable-oversight-intro/,,Maybe,https://open.spotify.com/episode/1mj9t01J9rEV7rHmNZqinQ?si=5c33eab2457241c9,Adam Jones,Article,2024,Can we scale human feedback for complex AI tasks?,,,Constraining dangerous AI capabilities,AGI Strategy,AGI Strategy - 4: Defence in Depth
AI companies are not on track to secure model weights - ,https://forum.effectivealtruism.org/posts/fJycPfYuBHKoai98q/ai-companies-are-not-on-track-to-secure-model-weights,,Maybe,,Jeffrey Ladish,Blog,2024,AI companies are not on track to secure model weights,,,Constraining dangerous AI capabilities,AGI Strategy,AGI Strategy - 4: Defence in Depth
Scheming reasoning evaluations - ,https://www.apolloresearch.ai/research/scheming-reasoning-evaluations,,Maybe,,Hobbhahn et al.,Blog,2024,Scheming reasoning evaluations,,,Constraining dangerous AI capabilities,AGI Strategy,AGI Strategy - 4: Defence in Depth
Societal Adaptation to Advanced AI - ,https://www.governance.ai/research-paper/societal-adaptation-to-advanced-ai,,Maybe,https://open.spotify.com/episode/0WLgPFD0xDSNjrg5vApb7u?si=7bce36c5bda3494a,"Jamie Bernardi, Gabriel Mukobi, Hilary Greaves, Lennart Heim, and Markus Anderljung",Paper,2024,Societal Adaptation to Advanced AI,,,Withstanding dangerous AI actions,AGI Strategy,AGI Strategy - 4: Defence in Depth
AI Safety Evaluations: An Explainer - ,https://cset.georgetown.edu/article/ai-safety-evaluations-an-explainer/,10,Maybe,,"Jessica Ji, Vikram Venkatram, and Steph Batalis",,2025,AI Safety Evaluations: An Explainer,2,"You can't effectively constrain dangerous AI capabilities without first detecting and measuring them. CSET researchers explain the difference between testing what AI models can output and how they affect real-world outcomes, and how this is used to measure risk.",Constraining dangerous AI capabilities,AGI Strategy,AGI Strategy - 4: Defence in Depth
Notes on Differential Technological Development - ,https://michaelnotebook.com/dtd/index.html,40,Maybe,,Michael Nielsen,,2024,Notes on Differential Technological Development,3,"Markets excel at safety when the risks hit consumers directly (e.g. unsafe aircraft), but fail with long-term, diffuse risks like climate change and AI threats. In this blog post, Nielsen proposes institutional changes that might accelerate defensive technologies faster than offensive ones.",Withstanding dangerous AI actions,AGI Strategy,AGI Strategy - 4: Defence in Depth
Beyond a Manhattan Project for Artificial General Intelligence - ,https://www.rand.org/pubs/commentary/2025/04/beyond-a-manhattan-project-for-artificial-general-intelligence.html,,Maybe,,Matt Chessen and Craig Martell,Blog,2025,Beyond a Manhattan Project for Artificial General Intelligence,4,,Preventing dangerous AI training,AGI Strategy,AGI Strategy - 4: Defence in Depth
How big could an “AI Manhattan Project” get? - ,https://epochai.substack.com/p/how-big-could-an-ai-manhattan-project,,Maybe,,Arden Berg and Anson Ho,Blog,2025,How big could an “AI Manhattan Project” get?,,,Stepping up,AGI Strategy,AGI Strategy - 5: Designing a Better Future
Intelsat as a Model for International AGI Governance - ,https://www.forethought.org/research/intelsat-as-a-model-for-international-agi-governance,,Maybe,,Will MacAskill and Rose Hadshar,Article,2025,Intelsat as a Model for International AGI Governance,,,Stepping up,AGI Strategy,AGI Strategy - 5: Designing a Better Future
Beyond a Manhattan Project for Artificial General Intelligence - ,https://www.rand.org/pubs/commentary/2025/04/beyond-a-manhattan-project-for-artificial-general-intelligence.html,,Maybe,,Matt Chessen and Craig Martell,Blog,2025,Beyond a Manhattan Project for Artificial General Intelligence,,,Stepping up,AGI Strategy,AGI Strategy - 5: Designing a Better Future
"""The Project, Situational Awareness"" - ",https://situational-awareness.ai/the-project/,,Maybe,,Leopold Aschenbrenner,Blog,2024,"The Project, Situational Awareness",,,Stepping up,AGI Strategy,AGI Strategy - 5: Designing a Better Future
Four Lessons from Historical Tech Regulation to Aid AI Policymaking - ,https://www.csis.org/analysis/four-lessons-historical-tech-regulation-aid-ai-policymaking,,Maybe,,Michael Frank,Article,2023,Four Lessons from Historical Tech Regulation to Aid AI Policymaking,,,Stepping up,AGI Strategy,AGI Strategy - 5: Designing a Better Future
Insights from Nuclear History for AI Governance - ,https://www.rand.org/pubs/perspectives/PEA3652-1.html,,Maybe,,Boudreaux et al.,Paper,2025,Insights from Nuclear History for AI Governance,,,Stepping up,AGI Strategy,AGI Strategy - 5: Designing a Better Future
Urging an International AI Treaty: An Open Letter - ,https://aitreaty.org/,,Maybe,,Misc,Website,2024,Urging an International AI Treaty: An Open Letter,,,Stepping up,AGI Strategy,AGI Strategy - 5: Designing a Better Future
Example Supplementary Resource - ,https://example.com/supplementary-reading,20,Supplementary,,Example Author,Blog,2025,Example Supplementary Resource,99,"This is an example of a supplementary resource that provides additional context but is not required reading for the core curriculum.",Building defences,AGI Strategy,AGI Strategy - 4: Defence in Depth
Example Optional Resource - ,https://example.com/optional-reading,10,Optional,,Another Author,Website,2024,Example Optional Resource,100,"This is an example of an optional resource that students can explore if they want to dive deeper into the topic.",Building defences,AGI Strategy,AGI Strategy - 4: Defence in Depth